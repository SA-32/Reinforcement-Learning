{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4928e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0f2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n_rows, n_cols, goal_state, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_states = n_rows * n_cols\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.n_actions = len(self.actions)\n",
    "\n",
    "        # Q-table: states Ã— actions\n",
    "        self.q = np.zeros((self.n_states, self.n_actions))\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def transition_reward(self, state, action):\n",
    "        row, col = divmod(state, self.n_cols)\n",
    "\n",
    "        if action == 'up':\n",
    "            next_row, next_col = max(row - 1, 0), col\n",
    "        elif action == 'down':\n",
    "            next_row, next_col = min(row + 1, self.n_rows - 1), col\n",
    "        elif action == 'left':\n",
    "            next_row, next_col = row, max(0, col - 1)\n",
    "        elif action == 'right':\n",
    "            next_row, next_col = row, min(col + 1, self.n_cols - 1)\n",
    "\n",
    "        next_state = next_row * self.n_cols + next_col\n",
    "\n",
    "        # Reward: +1 if goal, -1 otherwise\n",
    "        reward = 1 if next_state == self.goal_state else -1\n",
    "        return next_state, reward\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)  # explore\n",
    "        else:\n",
    "            return np.argmax(self.q[state])  # exploit\n",
    "\n",
    "    def TD_Learning(self, episodes=1000):\n",
    "        for _ in range(episodes):\n",
    "            state = random.randint(0, self.n_states - 1)  # start randomly\n",
    "            action = self.choose_action(state)\n",
    "\n",
    "            while state != self.goal_state:\n",
    "                next_state, reward = self.transition_reward(state, self.actions[action])\n",
    "                next_action = self.choose_action(next_state)\n",
    "\n",
    "                # SARSA update\n",
    "                td_target = reward + self.gamma * self.q[next_state, next_action]\n",
    "                td_error = td_target - self.q[state, action]\n",
    "                self.q[state, action] += self.alpha * td_error\n",
    "\n",
    "                state, action = next_state, next_action\n",
    "        return self.q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf00980",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Model(3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97919623",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = game.TD_Learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f491a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18123168  1.         -0.04947873 -0.69349188]\n",
      " [-0.59164326 -0.40336719 -0.32715104 -1.0337235 ]\n",
      " [-1.26382141 -1.23014436 -1.23674331 -1.43083328]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.72769698 -0.87419885  1.         -0.80086835]\n",
      " [-0.88826107 -0.99766319 -0.38311643 -0.87205654]\n",
      " [ 1.         -0.13039122 -0.04894784 -0.1278613 ]\n",
      " [-0.13242414 -0.80986381 -0.35417476 -1.03801188]\n",
      " [-1.3001924  -1.59844828 -1.16275811 -1.48836168]]\n"
     ]
    }
   ],
   "source": [
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c19597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
